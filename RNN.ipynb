{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "In this Notebook I will train a Recurrent Neural Network (RNN) on the articles of my watson data set. Then I let the RNN write a new article. \n",
    "\n",
    "I follow more or less this post: \n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gwehrm\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import ipynb\n",
    "import ipynb.fs.full.Classifier as cl#from https://github.com/ptnplanet/NLTK-Contributions/blob/master/ClassifierBasedGermanTagger/ClassifierBasedGermanTagger.py\n",
    "import random\n",
    "import pickle\n",
    "import keras\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tourismus-Professor pendelt mit Flugzeug zur A...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 22:15 28.03.19, 22:40</td>\n",
       "      <td>19</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Klima']</td>\n",
       "      <td>['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_title</td>\n",
       "      <td>no_author</td>\n",
       "      <td>no_date</td>\n",
       "      <td>no_comments</td>\n",
       "      <td>[]</td>\n",
       "      <td>['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anstatt mit Bus und Zug fahren mehr Menschen m...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:39</td>\n",
       "      <td>29</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Mobilit...</td>\n",
       "      <td>['\\nDer Ausbau des öffentlichen Verkehrs würde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Über 80'000 Franken bei Online-Bank N26 geklau...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:34</td>\n",
       "      <td>18</td>\n",
       "      <td>['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...</td>\n",
       "      <td>['\\nDie gefeierte Online-Bank N26 verspielt ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Der Wolf ist zurück – was auch Städter wissen ...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 16:19</td>\n",
       "      <td>45</td>\n",
       "      <td>['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']</td>\n",
       "      <td>['\\nDer gesetzliche Schutz des Wolfes wird der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     author  \\\n",
       "0  Tourismus-Professor pendelt mit Flugzeug zur A...  no_author   \n",
       "1                                           no_title  no_author   \n",
       "2  Anstatt mit Bus und Zug fahren mehr Menschen m...  no_author   \n",
       "3  Über 80'000 Franken bei Online-Bank N26 geklau...  no_author   \n",
       "4  Der Wolf ist zurück – was auch Städter wissen ...  no_author   \n",
       "\n",
       "                              date nmbr_comments  \\\n",
       "0  28.03.19, 22:15 28.03.19, 22:40            19   \n",
       "1                          no_date   no_comments   \n",
       "2                 28.03.19, 17:39             29   \n",
       "3                 28.03.19, 17:34             18   \n",
       "4                 28.03.19, 16:19             45   \n",
       "\n",
       "                                              themes  \\\n",
       "0     ['Schweiz', 'Gesellschaft & Politik', 'Klima']   \n",
       "1                                                 []   \n",
       "2  ['Schweiz', 'Gesellschaft & Politik', 'Mobilit...   \n",
       "3  ['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...   \n",
       "4   ['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']   \n",
       "\n",
       "                                             article  \n",
       "0  ['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...  \n",
       "1  ['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...  \n",
       "2  ['\\nDer Ausbau des öffentlichen Verkehrs würde...  \n",
       "3  ['\\nDie gefeierte Online-Bank N26 verspielt ge...  \n",
       "4  ['\\nDer gesetzliche Schutz des Wolfes wird der...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jacqueline Büchi</th>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  date  nmbr_comments  themes  article\n",
       "author                                                       \n",
       "Jacqueline Büchi    155   155            155     155      155"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"watson_schweiz.csv\",sep = \";\") \n",
    "display(data.head(5))\n",
    "\n",
    "# filter no_author\n",
    "data = data[-data['author'].str.contains(\"no_author\")]\n",
    "\n",
    "# filter authors <50 articles - in order to make \n",
    "data = data.groupby('author')\n",
    "data = data.filter(lambda x: len(x) > 152).reset_index(drop = True)\n",
    "display(data.groupby('author').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert the string, which is actually a list, to a nice string.\n",
    "def listtostring(l):\n",
    "    import ast\n",
    "    list1 = ast.literal_eval(l)\n",
    "    list2 = \" \".join(list1)\n",
    "    list3 = list2.strip()\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = data[\"article\"].apply(listtostring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seit über 30 Jahren kämpft Christoph Blocher für eine Abschaffung der Sommerzeit. Nun könnte sein Wunsch in Erfüllung gehen – ausgerechnet dank seiner Erzfeindin, der EU.  Es war einmal, in den frühen 80er-Jahren, ein Zürcher Nationalrat. Er hiess Christoph Blocher und er hatte einen Traum: Die Sommerzeit sollte weg! Eilends lancierte er eine Volksinitiative. Allerdings brachten er und seine Mitstreiter der Zürcher SVP die nötigen Unterschriften nicht zusammen – das Vorhaben scheiterte im Sammelstadium. \\nChristoph Blocher anno 1980. Bild: KEYSTONE In den darauffolgenden Jahren verschob sich der Fokus Blochers. Sein Kampf galt nun der Unabhängigkeit der Schweiz von der Europäischen Union. Indem er sich erfolgreich gegen den Beitritt zum Europäischen Wirtschaftsraum (EWR) zur Wehr setzte, avancierte er 1992 zur Ikone aller EU-Skeptiker. Den Kampf gegen die Sommerzeit führten derweil andere für ihn weiter. Insbesondere Yvette Estermann, SVP-Nationalrätin aus dem Kanton Luzern. Erstmals forderte sie 2010 in einer Motion: \\n Der Bundesrat lehnte das Begehren ab, das Parlament ebenso. Doch Estermann gab nicht auf: 2012, 2016, 2017 und 2018 stiess sie mit mehreren ähnlich lautenden Vorstössen nach. \\nYvette Estermann liess nicht locker. Bild: KEYSTONE Der Bundesrat speiste die unermüdliche SVP-Frau jeweils mit der Antwort ab, die Schweiz halte an der Sommerzeit fest, weil sie sonst zur «Zeitinsel» in Europa würde. Diese Situation hatte man in den Sommermonaten im Jahr 1980 schon einmal, als die Eidgenossenschaft im Gegensatz zu den umliegenden Ländern noch keine Sommerzeit eingeführt hatte. Und die Erfahrungen, die man damit machte, waren alles andere als gut: Probleme im Transportwesen, im Tourismus und in der Kommunikation seien unvermeidlich, schreibt der Bundesrat in seinen Antworten auf Estermanns Vorstösse. Unter anderem wären «regelmässige Missverständnisse bei Terminen» zu erwarten, sollte die Schweiz zeittechnisch noch einmal auf diese Weise ausscheren. Nur falls «eine Mehrheit der die Schweiz umgebenden Länder» die Zeitumstellung abschaffen sollte, werde sich die Schweiz diesen Schritt auch überlegen, hielt der Bundesrat weiter fest. 6,783 \\n Dies dürfte mit dem heutigen Tag Realität werden. «Die Menschen wollen das, wir machen das», kündigte der EU-Kommissionspräsident Jean-Claude Juncker im ZDF-«Morgenmagazin» das Ende der Zeitumstellung an. Damit ist wohl auch in der Schweiz bald fertig an der Uhr gedreht. Die Ironie der Geschichte: Ausgerechnet die Europäische Union dürfte Christoph Blocher seinen langgehegten Traum erfüllen. Alle Gegener der Sommerzeitumstellung freuen sich! Endlich ein positiver Signal aus Brüssel... pic.twitter.com/5c0HLREaFJ \\n \\nVideo: srf Der Umbau in eine nachhaltige Wirtschaft schafft ideale Voraussetzungen für Unternehmer, Handwerker und Bauern. Was soll also der Krieg gegen vermeintliche «Klima-Kindersoldaten»? Nicolas Hayek war einer der bedeutendsten Schweizer Unternehmer in der zweiten Hälfte des letzten Jahrhunderts. Er hat die Uhrenindustrie gerettet und verschiedene Unternehmen saniert. In den Achtzigerjahren hatte Hayek eine Idee: Er wollte den Smart, ein Elektroauto für den Stadtverkehr, auf den Markt bringen.  Aus heutiger Sicht war Hayeks Idee visionär. Hätte er Erfolg gehabt, würde Tesla heute vielleicht nicht in der Wüste von Nevada, sondern im Berner Seeland gefertigt. Hayek zerbrach am …'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec model for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_split = articles.apply(lambda x: re.split(\"\\.|\\!|\\\\?\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list again\n",
    "texts = []\n",
    "for i in articles_split: # loop in loop, since each sentence needs to be seperately added\n",
    "    for s in i:\n",
    "        texts.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  [gensim.utils.simple_preprocess(_) for _ in texts] # do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1122 20:04:59.974195 18560 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words: 16764\n",
      "The first 10 words in the vocabularies: ['böse', 'avocado', 'liebe', 'geht', 'es', 'nach', 'autorin', 'kathrin', 'hartmann', 'soll']\n"
     ]
    }
   ],
   "source": [
    "sg_ = 1 # the training algorithm. If sg=0, CBOW is used. Otherwise (sg=1), skip-gram is employed.\n",
    "alg = 'CBOW' if sg_ == 0 else 'sg'\n",
    "size_ = 10 #  the dimensionality of the feature vectors\n",
    "window_ = 2 # the context size or the maximum distance between the current and predicted word\n",
    "\n",
    "\n",
    "model_watson = Word2Vec(sentences, size=100, window=5, min_count=1, workers=8)\n",
    "words = list(model_watson.wv.vocab.keys())\n",
    "print(f\"The number of words: {len(words)}\")\n",
    "print(f\"The first 10 words in the vocabularies: {words[0:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer object\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                              filters = '#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                                              lower = True,\n",
    "                                              split = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer to text\n",
    "tokenizer.fit_on_texts(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "# Iterate through the sequences of tokens\n",
    "for seq in sequences:\n",
    "\n",
    "    # Create multiple training examples from each sequence\n",
    "    for i in range(training_length, len(seq)):\n",
    "        \n",
    "        # Extract the features and label\n",
    "        extract = seq[i - training_length:i + 1]\n",
    "\n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "        \n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(137474, 26507)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in vocabulary\n",
    "num_words = len(tokenizer.index_word)+1\n",
    "word_index = tokenizer.index_word\n",
    "\n",
    "\n",
    "display(num_words)\n",
    "\n",
    "# empty array for labels\n",
    "label_array = np.zeros((len(features), num_words),dtype = np.int8)\n",
    "\n",
    "# one hot encode labels\n",
    "for example_index, word, in enumerate(labels):\n",
    "    label_array[example_index, word]= 1\n",
    "    \n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# with gzip.open(r'C:\\Users\\gwehrm\\Downloads\\cc.de.300.vec.gz', 'rb') as f_in:\n",
    "#     with open(r'C:\\Users\\gwehrm\\Documents\\Repos\\watson_analysis\\german_emb.txt', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New matrix to hold word embeddings\n",
    "embedding_matrix = np.zeros((num_words, model_watson.vector_size))\n",
    "\n",
    "for i, word in enumerate(word_index.keys()):\n",
    "    # Look up the word embedding\n",
    "    if isinstance(i, int):\n",
    "        continue\n",
    "    vector = model_watson[str(word)]\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bda150c59170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m               \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m               \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m               \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m               \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m               mask_zero=True))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# build RNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('models/model.h5', save_best_only = True, \n",
    "                             save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137474, 50)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(137474, 26507)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features.shape)\n",
    "display(label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train /test \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label_array\n",
    "                                   ,test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1121 10:54:07.555055  4704 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109979 samples, validate on 27495 samples\n",
      "Epoch 1/150\n",
      "109979/109979 [==============================] - 178s 2ms/step - loss: 10.1612 - accuracy: 0.0117 - val_loss: 10.1380 - val_accuracy: 0.0375\n",
      "Epoch 2/150\n",
      "109979/109979 [==============================] - 177s 2ms/step - loss: 10.1124 - accuracy: 0.0377 - val_loss: 10.0970 - val_accuracy: 0.0375\n",
      "Epoch 3/150\n",
      "109979/109979 [==============================] - 172s 2ms/step - loss: 10.0668 - accuracy: 0.0377 - val_loss: 10.0567 - val_accuracy: 0.0375\n",
      "Epoch 4/150\n",
      "109979/109979 [==============================] - 176s 2ms/step - loss: 10.0220 - accuracy: 0.0377 - val_loss: 10.0170 - val_accuracy: 0.0375\n",
      "Epoch 5/150\n",
      "109979/109979 [==============================] - 170s 2ms/step - loss: 9.9777 - accuracy: 0.0377 - val_loss: 9.9778 - val_accuracy: 0.0375\n",
      "Epoch 6/150\n",
      "109979/109979 [==============================] - 172s 2ms/step - loss: 9.9339 - accuracy: 0.0377 - val_loss: 9.9390 - val_accuracy: 0.0375\n",
      "Epoch 7/150\n",
      "109979/109979 [==============================] - 190s 2ms/step - loss: 9.8907 - accuracy: 0.0377 - val_loss: 9.9006 - val_accuracy: 0.0375\n",
      "Epoch 8/150\n",
      "109979/109979 [==============================] - 180s 2ms/step - loss: 9.8479 - accuracy: 0.0377 - val_loss: 9.8625 - val_accuracy: 0.0375\n",
      "Epoch 9/150\n",
      "109979/109979 [==============================] - 186s 2ms/step - loss: 9.8057 - accuracy: 0.0377 - val_loss: 9.8247 - val_accuracy: 0.0375\n",
      "Epoch 10/150\n",
      "109979/109979 [==============================] - 200s 2ms/step - loss: 9.7639 - accuracy: 0.0377 - val_loss: 9.7876 - val_accuracy: 0.0375\n",
      "Epoch 11/150\n",
      "109979/109979 [==============================] - 197s 2ms/step - loss: 9.7225 - accuracy: 0.0377 - val_loss: 9.7507 - val_accuracy: 0.0375\n",
      "Epoch 12/150\n",
      "109979/109979 [==============================] - 183s 2ms/step - loss: 9.6817 - accuracy: 0.0377 - val_loss: 9.7142 - val_accuracy: 0.0375\n",
      "Epoch 13/150\n",
      "109979/109979 [==============================] - 197s 2ms/step - loss: 9.6413 - accuracy: 0.0377 - val_loss: 9.6780 - val_accuracy: 0.0375\n",
      "Epoch 14/150\n",
      "109979/109979 [==============================] - 209s 2ms/step - loss: 9.6014 - accuracy: 0.0377 - val_loss: 9.6422 - val_accuracy: 0.0375\n",
      "Epoch 15/150\n",
      "109979/109979 [==============================] - 369s 3ms/step - loss: 9.5619 - accuracy: 0.0377 - val_loss: 9.6069 - val_accuracy: 0.0375\n",
      "Epoch 16/150\n",
      "109979/109979 [==============================] - 277s 3ms/step - loss: 9.5228 - accuracy: 0.0377 - val_loss: 9.5720 - val_accuracy: 0.0375\n",
      "Epoch 17/150\n",
      "109979/109979 [==============================] - 196s 2ms/step - loss: 9.4843 - accuracy: 0.0377 - val_loss: 9.5373 - val_accuracy: 0.0375\n",
      "Epoch 18/150\n",
      "109979/109979 [==============================] - 214s 2ms/step - loss: 9.4461 - accuracy: 0.0377 - val_loss: 9.5030 - val_accuracy: 0.0375\n",
      "Epoch 19/150\n",
      "109979/109979 [==============================] - 212s 2ms/step - loss: 9.4084 - accuracy: 0.0377 - val_loss: 9.4692 - val_accuracy: 0.0375\n",
      "Epoch 20/150\n",
      "109979/109979 [==============================] - 188s 2ms/step - loss: 9.3712 - accuracy: 0.0377 - val_loss: 9.4359 - val_accuracy: 0.0375\n",
      "Epoch 21/150\n",
      "109979/109979 [==============================] - 209s 2ms/step - loss: 9.3344 - accuracy: 0.0377 - val_loss: 9.4027 - val_accuracy: 0.0375\n",
      "Epoch 22/150\n",
      "109979/109979 [==============================] - 262s 2ms/step - loss: 9.2980 - accuracy: 0.0377 - val_loss: 9.3701 - val_accuracy: 0.0375\n",
      "Epoch 23/150\n",
      "109979/109979 [==============================] - 318s 3ms/step - loss: 9.2621 - accuracy: 0.0377 - val_loss: 9.3377 - val_accuracy: 0.0375\n",
      "Epoch 24/150\n",
      "109979/109979 [==============================] - 323s 3ms/step - loss: 9.2267 - accuracy: 0.0377 - val_loss: 9.3059 - val_accuracy: 0.0375\n",
      "Epoch 25/150\n",
      "109979/109979 [==============================] - 350s 3ms/step - loss: 9.1916 - accuracy: 0.0377 - val_loss: 9.2744 - val_accuracy: 0.0375\n",
      "Epoch 26/150\n",
      "109979/109979 [==============================] - 357s 3ms/step - loss: 9.1571 - accuracy: 0.0377 - val_loss: 9.2432 - val_accuracy: 0.0375\n",
      "Epoch 27/150\n",
      "109979/109979 [==============================] - 279s 3ms/step - loss: 9.1230 - accuracy: 0.0377 - val_loss: 9.2127 - val_accuracy: 0.0375\n",
      "Epoch 28/150\n",
      "109979/109979 [==============================] - 194s 2ms/step - loss: 9.0892 - accuracy: 0.0377 - val_loss: 9.1823 - val_accuracy: 0.0375\n",
      "Epoch 29/150\n",
      "109979/109979 [==============================] - 208s 2ms/step - loss: 9.0560 - accuracy: 0.0377 - val_loss: 9.1523 - val_accuracy: 0.0375\n",
      "Epoch 30/150\n",
      "109979/109979 [==============================] - 221s 2ms/step - loss: 9.0232 - accuracy: 0.0377 - val_loss: 9.1231 - val_accuracy: 0.0375\n",
      "Epoch 31/150\n",
      "109979/109979 [==============================] - 173s 2ms/step - loss: 8.9909 - accuracy: 0.0377 - val_loss: 9.0940 - val_accuracy: 0.0375\n",
      "Epoch 32/150\n",
      "109979/109979 [==============================] - 184s 2ms/step - loss: 8.9590 - accuracy: 0.0377 - val_loss: 9.0654 - val_accuracy: 0.0375\n",
      "Epoch 33/150\n",
      "109979/109979 [==============================] - 190s 2ms/step - loss: 8.9276 - accuracy: 0.0377 - val_loss: 9.0370 - val_accuracy: 0.0375\n",
      "Epoch 34/150\n",
      "109979/109979 [==============================] - 179s 2ms/step - loss: 8.8967 - accuracy: 0.0377 - val_loss: 9.0092 - val_accuracy: 0.0375\n",
      "Epoch 35/150\n",
      "109979/109979 [==============================] - 187s 2ms/step - loss: 8.8662 - accuracy: 0.0377 - val_loss: 8.9818 - val_accuracy: 0.0375\n",
      "Epoch 36/150\n",
      "109979/109979 [==============================] - 181s 2ms/step - loss: 8.8361 - accuracy: 0.0377 - val_loss: 8.9550 - val_accuracy: 0.0375\n",
      "Epoch 37/150\n",
      "109979/109979 [==============================] - 204s 2ms/step - loss: 8.8065 - accuracy: 0.0377 - val_loss: 8.9284 - val_accuracy: 0.0375\n",
      "Epoch 38/150\n",
      "109979/109979 [==============================] - 204s 2ms/step - loss: 8.7774 - accuracy: 0.0377 - val_loss: 8.9024 - val_accuracy: 0.0375\n",
      "Epoch 39/150\n",
      "109979/109979 [==============================] - 201s 2ms/step - loss: 8.7487 - accuracy: 0.0377 - val_loss: 8.8767 - val_accuracy: 0.0375\n",
      "Epoch 40/150\n",
      "109979/109979 [==============================] - 186s 2ms/step - loss: 8.7205 - accuracy: 0.0377 - val_loss: 8.8516 - val_accuracy: 0.0375\n",
      "Epoch 41/150\n",
      "109979/109979 [==============================] - 174s 2ms/step - loss: 8.6928 - accuracy: 0.0377 - val_loss: 8.8268 - val_accuracy: 0.0375\n",
      "Epoch 42/150\n",
      "109979/109979 [==============================] - 179s 2ms/step - loss: 8.6655 - accuracy: 0.0377 - val_loss: 8.8023 - val_accuracy: 0.0375\n",
      "Epoch 43/150\n",
      "109979/109979 [==============================] - 178s 2ms/step - loss: 8.6387 - accuracy: 0.0377 - val_loss: 8.7785 - val_accuracy: 0.0375\n",
      "Epoch 44/150\n",
      "109979/109979 [==============================] - 175s 2ms/step - loss: 8.6124 - accuracy: 0.0377 - val_loss: 8.7552 - val_accuracy: 0.0375\n",
      "Epoch 45/150\n",
      "109979/109979 [==============================] - 180s 2ms/step - loss: 8.5865 - accuracy: 0.0377 - val_loss: 8.7321 - val_accuracy: 0.0375\n",
      "Epoch 46/150\n",
      "109979/109979 [==============================] - 181s 2ms/step - loss: 8.5611 - accuracy: 0.0377 - val_loss: 8.7097 - val_accuracy: 0.0375\n",
      "Epoch 47/150\n",
      "109979/109979 [==============================] - 173s 2ms/step - loss: 8.5361 - accuracy: 0.0377 - val_loss: 8.6874 - val_accuracy: 0.0375\n",
      "Epoch 48/150\n",
      "109979/109979 [==============================] - 174s 2ms/step - loss: 8.5116 - accuracy: 0.0377 - val_loss: 8.6657 - val_accuracy: 0.0375\n",
      "Epoch 49/150\n",
      "109979/109979 [==============================] - 175s 2ms/step - loss: 8.4876 - accuracy: 0.0377 - val_loss: 8.6446 - val_accuracy: 0.0375\n",
      "Epoch 50/150\n",
      "109979/109979 [==============================] - 194s 2ms/step - loss: 8.4640 - accuracy: 0.0377 - val_loss: 8.6239 - val_accuracy: 0.0375\n",
      "Epoch 51/150\n",
      "109979/109979 [==============================] - 196s 2ms/step - loss: 8.4409 - accuracy: 0.0377 - val_loss: 8.6036 - val_accuracy: 0.0375\n",
      "Epoch 52/150\n",
      "109979/109979 [==============================] - 176s 2ms/step - loss: 8.4183 - accuracy: 0.0377 - val_loss: 8.5835 - val_accuracy: 0.0375\n",
      "Epoch 53/150\n",
      "109979/109979 [==============================] - 179s 2ms/step - loss: 8.3960 - accuracy: 0.0377 - val_loss: 8.5641 - val_accuracy: 0.0375\n",
      "Epoch 54/150\n",
      " 65536/109979 [================>.............] - ETA: 1:19 - loss: 8.3778 - accuracy: 0.0372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c21d4c74c49b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train,  y_train, \n",
    "                    batch_size=2048, epochs=150,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was stopped, since the val_accuracy didnt increase further ( I didnt do it earlier, since I ate lunch:) ). The accuracy is pretty low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c92ced206c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Load in model and evaluate on validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load in model and evaluate on validation data\n",
    "model = keras.models.load_model('models/model.h5')\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
