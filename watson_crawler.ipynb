{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Crawler\n",
    "In this notebook, I make a crawler for the news portal Watson (https://www.watson.ch/)\n",
    "Goal is to get for every article\n",
    "* title\n",
    "* author\n",
    "* publishing date\n",
    "* number of comments\n",
    "* themes\n",
    "* article\n",
    "So far I will do it only for the \"Ressort Schweiz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlwt\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classes \n",
    "class link_it(): # creates the new link for the sites\n",
    "    def __init__(self,url, link):\n",
    "        self.url = url\n",
    "        self.link = urljoin(url,link)\n",
    "\n",
    "class crawled_elements(): # stores the crawled elements\n",
    "    def __init__(self, title, author, date, comments, themes, article):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.date = date\n",
    "        self.comments = comments\n",
    "        self.themes = themes\n",
    "        self.article = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual crawler\n",
    "class Fetcher():\n",
    "    def fetch(self,url):\n",
    "        links = []\n",
    "        while url != \"\":\n",
    "            r = requests.get(url)\n",
    "            time.sleep(2) \n",
    "            print(r)\n",
    "            doc = BeautifulSoup(r.text, \"html.parser\")\n",
    "            for row in doc.select(\".teaserlink\"):\n",
    "                link = row.attrs[\"href\"]\n",
    "                l = link_it(url, link)\n",
    "                links.append(l)\n",
    "            print(l.link)\n",
    "            # error exit or somewhat in the last line\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    next_url = link_it(url, doc.select_one(\".next a\")[\"href\"])\n",
    "                    url = next_url.link\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    url = \"\"\n",
    "                break\n",
    "\n",
    "\n",
    "            # if doc.select_one(\".next a\")[\"href\"] ==  None:\n",
    "            #     url = \"\"\n",
    "            # else:\n",
    "            #     next_url = link_it(url, doc.select_one(\".next a\")[\"href\"])\n",
    "            #     url = next_url.link\n",
    "\n",
    "        crawled = []\n",
    "        for link in links:\n",
    "            print(link.link)\n",
    "            r = requests.get(link.link)\n",
    "            time.sleep(2)\n",
    "            doc = BeautifulSoup(r.text, \"html.parser\")\n",
    "            while True:\n",
    "                try:\n",
    "                    title = doc.select_one(\".maintitle\").text\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    title = \"no_title\"\n",
    "                break\n",
    "            print(\"title:\", title)\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    date = doc.select_one(\".publish_date\").text\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    date = \"no_date\"\n",
    "                break\n",
    "\n",
    "            print(\"date:\", date)\n",
    "            while True:\n",
    "                try:\n",
    "                    author = doc.select_one(\".card h6\").text\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    author = \"no_author\"\n",
    "                break\n",
    "            print(\"author:\", author)\n",
    "            while True:\n",
    "                try:\n",
    "                    comments = doc.select_one(\".commentButton.standard-button .number\").text\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    comments = \"no_comments\"\n",
    "                break\n",
    "            print(\"comments:\", comments)\n",
    "            themes = []\n",
    "            while True:\n",
    "                try:\n",
    "                    tags = doc.select(\".widget.tags ul a\")\n",
    "                    for element in tags:\n",
    "                        themes.append(element.text)\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    themes.append(\"no_themes\")\n",
    "                break\n",
    "\n",
    "            print(\"themes:\", themes)\n",
    "            article = []\n",
    "            while True:\n",
    "                try:\n",
    "                    text = doc.select(\"p\")\n",
    "                    for paragraph in text:\n",
    "                        article.append(paragraph.text)\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    article.append(\"no_article\")\n",
    "                break\n",
    "            print(article)\n",
    "            crawled_article = crawled_elements(title,author,date,comments,themes,article)\n",
    "            crawled.append(crawled_article)\n",
    "        return(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to actually execute the crawler \n",
    "# be aware that it takes a while;)\n",
    "\n",
    "x = Fetcher()\n",
    "url = \"https://www.watson.ch/Schweiz\"\n",
    "t = x.fetch(url)\n",
    "\n",
    "with open(\"watson_schweiz.csv\", \"w\", newline=\"\", encoding='utf-8') as csvfile:\n",
    "    countrywriter = csv.writer(csvfile, delimiter = \";\",\n",
    "                           quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    countrywriter.writerow([\"title\", \"author\", \"date\", \"nmbr_comments\", \"themes\", \"article\"])\n",
    "    for article in t:\n",
    "        countrywriter.writerow([article.title, article.author, article.date, article.comments, article.themes, article.article])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
