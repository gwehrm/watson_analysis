{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journalist Identification\n",
    "\n",
    "I have a bunch of news articles from https://www.watson.ch/Schweiz/ ...\n",
    "\n",
    "All of them are of course well written, interesting, and just pure outbursts of originality. Well, I want to put it to a test.\n",
    "How to do that? The goal is to train an Naive Bayes algorithm that predicts the author based on text snippets.  \n",
    "\n",
    "So the question is:  \n",
    "**Is it possible to predict the author of a news article based on the text?**\n",
    "\n",
    "### Limitations:\n",
    "Journalists tend to specialize in certain topics, which might lead to the case that they use certain words because of their specialization and not because of their writing style. So the algorithm identifies the Journalists not by their writing style, but because of their specialization. To minimize this error, I only took articles from one topic (here Switzerland). Still, with the interpretation of the results, one has to be careful. As always!\n",
    "\n",
    "With this in mind: let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import ipynb\n",
    "import ipynb.fs.full.Classifier as cl#from https://github.com/ptnplanet/NLTK-Contributions/blob/master/ClassifierBasedGermanTagger/ClassifierBasedGermanTagger.py\n",
    "import random\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tourismus-Professor pendelt mit Flugzeug zur A...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 22:15 28.03.19, 22:40</td>\n",
       "      <td>19</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Klima']</td>\n",
       "      <td>['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_title</td>\n",
       "      <td>no_author</td>\n",
       "      <td>no_date</td>\n",
       "      <td>no_comments</td>\n",
       "      <td>[]</td>\n",
       "      <td>['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anstatt mit Bus und Zug fahren mehr Menschen m...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:39</td>\n",
       "      <td>29</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Mobilit...</td>\n",
       "      <td>['\\nDer Ausbau des öffentlichen Verkehrs würde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Über 80'000 Franken bei Online-Bank N26 geklau...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:34</td>\n",
       "      <td>18</td>\n",
       "      <td>['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...</td>\n",
       "      <td>['\\nDie gefeierte Online-Bank N26 verspielt ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Der Wolf ist zurück – was auch Städter wissen ...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 16:19</td>\n",
       "      <td>45</td>\n",
       "      <td>['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']</td>\n",
       "      <td>['\\nDer gesetzliche Schutz des Wolfes wird der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     author  \\\n",
       "0  Tourismus-Professor pendelt mit Flugzeug zur A...  no_author   \n",
       "1                                           no_title  no_author   \n",
       "2  Anstatt mit Bus und Zug fahren mehr Menschen m...  no_author   \n",
       "3  Über 80'000 Franken bei Online-Bank N26 geklau...  no_author   \n",
       "4  Der Wolf ist zurück – was auch Städter wissen ...  no_author   \n",
       "\n",
       "                              date nmbr_comments  \\\n",
       "0  28.03.19, 22:15 28.03.19, 22:40            19   \n",
       "1                          no_date   no_comments   \n",
       "2                 28.03.19, 17:39             29   \n",
       "3                 28.03.19, 17:34             18   \n",
       "4                 28.03.19, 16:19             45   \n",
       "\n",
       "                                              themes  \\\n",
       "0     ['Schweiz', 'Gesellschaft & Politik', 'Klima']   \n",
       "1                                                 []   \n",
       "2  ['Schweiz', 'Gesellschaft & Politik', 'Mobilit...   \n",
       "3  ['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...   \n",
       "4   ['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']   \n",
       "\n",
       "                                             article  \n",
       "0  ['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...  \n",
       "1  ['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...  \n",
       "2  ['\\nDer Ausbau des öffentlichen Verkehrs würde...  \n",
       "3  ['\\nDie gefeierte Online-Bank N26 verspielt ge...  \n",
       "4  ['\\nDer gesetzliche Schutz des Wolfes wird der...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7203</td>\n",
       "      <td>60</td>\n",
       "      <td>7211</td>\n",
       "      <td>288</td>\n",
       "      <td>4000</td>\n",
       "      <td>7214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>no_title</td>\n",
       "      <td>no_author</td>\n",
       "      <td>no_date</td>\n",
       "      <td>0</td>\n",
       "      <td>['Schweiz']</td>\n",
       "      <td>['Sorry, the page you are looking for is curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>5741</td>\n",
       "      <td>12</td>\n",
       "      <td>715</td>\n",
       "      <td>164</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title     author     date nmbr_comments       themes  \\\n",
       "count       7232       7232     7232          7232         7232   \n",
       "unique      7203         60     7211           288         4000   \n",
       "top     no_title  no_author  no_date             0  ['Schweiz']   \n",
       "freq          15       5741       12           715          164   \n",
       "\n",
       "                                                  article  \n",
       "count                                                7232  \n",
       "unique                                               7214  \n",
       "top     ['Sorry, the page you are looking for is curre...  \n",
       "freq                                                    9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"watson_schweiz.csv\",sep = \";\") \n",
    "display(data.head(5))\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first look, we see already some issues, so lets further visualise the data to see what's next. Since I'm only interested in article text and the author, I will only have a look at these columns.\n",
    "\n",
    "Also, I'm gonna encode the names of the journalists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article\n",
       "author         \n",
       "1            63\n",
       "5           113\n",
       "7           149\n",
       "15          104\n",
       "16          152\n",
       "19          155\n",
       "28           52\n",
       "42          133\n",
       "49           99\n",
       "57           60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_reduced = data.filter(items=['author', 'article'])\n",
    "# filter no_author\n",
    "data_reduced = data_reduced[-data_reduced['author'].str.contains(\"no_author\")]\n",
    "# authors_article = data_reduced.groupby('author').count().reset_index()\n",
    "# for simplicity I will reduce the number of authors. I set a threshold of minimum 50 articles \n",
    "\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "data_reduced[\"author\"] = labelencoder.fit_transform(data_reduced[\"author\"])\n",
    "\n",
    "\n",
    "g = data_reduced.groupby('author')\n",
    "data_reduced = g.filter(lambda x: len(x) > 50).reset_index(drop = True)\n",
    "display(data_reduced.groupby('author').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already way better - Only the authors with more than 50 articles are left. The next steps contain the preparation of the text itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "for index,s in enumerate(data_reduced[\"article\"]):\n",
    "    exclude = set(string.punctuation)\n",
    "    data_reduced[\"article\"][index] = ''.join(ch for ch in s if ch not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the lemmatization on the whole dataset, I remove the Stopwords. It leaves less words to process\n",
    "\n",
    "Stopwords are usually words that do not really contain much valuable information, but frequently occur, about a text.\n",
    "\n",
    "Examples:\n",
    "- die\n",
    "- dort\n",
    "- zu\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gwehrm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dowloading the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifiy german\n",
    "from nltk.corpus import stopwords\n",
    "# and check them\n",
    "stopwords.words('german')[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare for the lemmatization - I followed the steps according to https://github.com/WZBSocialScienceCenter/germalemma/blob/master/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dowloaded corpus \n",
    "corp = nltk.corpus.ConllCorpusReader('C:\\\\Users\\\\gwehrm\\\\Documents', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                     encoding='utf-8')\n",
    "\n",
    "tagged_sents = list(corp.tagged_sents())\n",
    "random.shuffle(tagged_sents)\n",
    "\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "# from ClassifierBasedGermanTagger\n",
    "#train the classifier ()\n",
    "tagger = cl.ClassifierBasedGermanTagger(train=train_sents)\n",
    "\n",
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "\n",
    "accuracy = tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for index,article in enumerate(data_reduced[\"article\"]):\n",
    "    data_reduced[\"article\"][index]= tagger.tag([word for word in article.split() if word.lower() not in stopwords.words('german')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to write the trained tagger on the disk that its not necessary to train it each time\n",
    "\n",
    "# with open('nltk_german_classifier_data.pickle', 'wb') as f:\n",
    "#     pickle.dump(tagger, f, protocol=2)\n",
    "#     #to load\n",
    "    # with open('nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "#     tagger = pickle.load(f)\n",
    "# print(index)\n",
    "# data_reduced.iloc[index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "# passing the word and the POS tag \n",
    "for index, tos in enumerate(data_reduced[\"article\"]):\n",
    "    article_w=[]\n",
    "    for i in tos:\n",
    "        try:\n",
    "            word, N = i\n",
    "            lemma = lemmatizer.find_lemma(word,N)\n",
    "            article_w.append(lemma)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    data_reduced.at[index,\"article\"] = article_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv to load in different setting\n",
    "data_reduced.to_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_reduced[\"author\"]\n",
    "X = data_reduced[\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(X):\n",
    "    X[index] = ' '.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                   ,test_size=0.2, random_state=1234)\n",
    "\n",
    "# defining the bag-of-words transformer on the text-processed corpus # i.e., text_process() declared in II is executed...\n",
    "bow_transformer=CountVectorizer().fit(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_train=bow_transformer.transform(X_train)#ONLY TRAINING DATA\n",
    "\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_test=bow_transformer.transform(X_test)#TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# instantiating the model with Multinomial Naive Bayes..\n",
    "model = MultinomialNB()\n",
    "# training the model...\n",
    "model = model.fit(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4861111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(text_bow_train, y_train)\n",
    "model.score(text_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.05      0.10        19\n",
      "          5       0.92      0.48      0.63        25\n",
      "          7       0.42      0.90      0.57        30\n",
      "         15       0.57      0.24      0.33        17\n",
      "         16       0.33      0.52      0.41        21\n",
      "         19       0.45      0.69      0.55        42\n",
      "         28       0.00      0.00      0.00        10\n",
      "         42       0.83      0.62      0.71        24\n",
      "         49       0.33      0.31      0.32        16\n",
      "         57       1.00      0.08      0.15        12\n",
      "\n",
      "avg / total       0.59      0.49      0.45       216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "# getting the predictions of the Validation Set...\n",
    "predictions = model.predict(text_bow_test)\n",
    "# getting the Precision, Recall, F1-Score\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 1  0  7  0  6  5  0  0  0  0]\n",
      " [ 0 12  1  0  7  4  0  0  1  0]\n",
      " [ 0  0 27  0  0  2  0  0  1  0]\n",
      " [ 0  0  4  4  6  2  0  0  1  0]\n",
      " [ 0  1  4  1 11  4  0  0  0  0]\n",
      " [ 0  0  9  0  0 29  0  3  1  0]\n",
      " [ 0  0  0  1  2  6  0  0  1  0]\n",
      " [ 0  0  5  0  0  2  0 15  2  0]\n",
      " [ 0  0  5  0  1  5  0  0  5  0]\n",
      " [ 0  0  2  1  0  5  0  0  3  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn! You can identify journalists based on their articles. Some better than others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
