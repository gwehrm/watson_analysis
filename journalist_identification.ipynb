{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journalist Identification\n",
    "\n",
    "I have a bunch of news articles from https://www.watson.ch/Schweiz/ ...\n",
    "\n",
    "All of them are of course well written, interesting, and just pure outbursts of originality. Well, I want to put it to a test.\n",
    "How to do that? The goal is to train an Naive Bayes algorithm that predicts the author based on text snippets.  \n",
    "\n",
    "So the question is:  \n",
    "**Is it possible to predict the author of a news article based on the text?**\n",
    "\n",
    "### Limitations:\n",
    "Journalists tend to specialize in certain topics, which might lead to the case that they use certain words because of their specialization and not because of their writing style. So the algorithm identifies the Journalists not by their writing style, but because of their specialization. To minimize this error, I only took articles from one topic (here Switzerland). Still, with the interpretation of the results, one has to be careful. As always!\n",
    "\n",
    "With this in mind: let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import ipynb\n",
    "import ipynb.fs.full.Classifier as cl#from https://github.com/ptnplanet/NLTK-Contributions/blob/master/ClassifierBasedGermanTagger/ClassifierBasedGermanTagger.py\n",
    "import random\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tourismus-Professor pendelt mit Flugzeug zur A...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 22:15 28.03.19, 22:40</td>\n",
       "      <td>19</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Klima']</td>\n",
       "      <td>['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_title</td>\n",
       "      <td>no_author</td>\n",
       "      <td>no_date</td>\n",
       "      <td>no_comments</td>\n",
       "      <td>[]</td>\n",
       "      <td>['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anstatt mit Bus und Zug fahren mehr Menschen m...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:39</td>\n",
       "      <td>29</td>\n",
       "      <td>['Schweiz', 'Gesellschaft &amp; Politik', 'Mobilit...</td>\n",
       "      <td>['\\nDer Ausbau des öffentlichen Verkehrs würde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Über 80'000 Franken bei Online-Bank N26 geklau...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 17:34</td>\n",
       "      <td>18</td>\n",
       "      <td>['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...</td>\n",
       "      <td>['\\nDie gefeierte Online-Bank N26 verspielt ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Der Wolf ist zurück – was auch Städter wissen ...</td>\n",
       "      <td>no_author</td>\n",
       "      <td>28.03.19, 16:19</td>\n",
       "      <td>45</td>\n",
       "      <td>['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']</td>\n",
       "      <td>['\\nDer gesetzliche Schutz des Wolfes wird der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     author  \\\n",
       "0  Tourismus-Professor pendelt mit Flugzeug zur A...  no_author   \n",
       "1                                           no_title  no_author   \n",
       "2  Anstatt mit Bus und Zug fahren mehr Menschen m...  no_author   \n",
       "3  Über 80'000 Franken bei Online-Bank N26 geklau...  no_author   \n",
       "4  Der Wolf ist zurück – was auch Städter wissen ...  no_author   \n",
       "\n",
       "                              date nmbr_comments  \\\n",
       "0  28.03.19, 22:15 28.03.19, 22:40            19   \n",
       "1                          no_date   no_comments   \n",
       "2                 28.03.19, 17:39             29   \n",
       "3                 28.03.19, 17:34             18   \n",
       "4                 28.03.19, 16:19             45   \n",
       "\n",
       "                                              themes  \\\n",
       "0     ['Schweiz', 'Gesellschaft & Politik', 'Klima']   \n",
       "1                                                 []   \n",
       "2  ['Schweiz', 'Gesellschaft & Politik', 'Mobilit...   \n",
       "3  ['Digital', 'Schweiz', 'Datenschutz', 'Deutsch...   \n",
       "4   ['Schweiz', 'Wissen', 'Aargau', 'Natur', 'Tier']   \n",
       "\n",
       "                                             article  \n",
       "0  ['Naaa, wie kommt ihr so zur Uni? Mit dem Fahr...  \n",
       "1  ['\\r\\n\\t\\tMit deiner Anmeldung erklärst du dic...  \n",
       "2  ['\\nDer Ausbau des öffentlichen Verkehrs würde...  \n",
       "3  ['\\nDie gefeierte Online-Bank N26 verspielt ge...  \n",
       "4  ['\\nDer gesetzliche Schutz des Wolfes wird der...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>nmbr_comments</th>\n",
       "      <th>themes</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "      <td>7232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7203</td>\n",
       "      <td>60</td>\n",
       "      <td>7211</td>\n",
       "      <td>288</td>\n",
       "      <td>4000</td>\n",
       "      <td>7214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>no_title</td>\n",
       "      <td>no_author</td>\n",
       "      <td>no_date</td>\n",
       "      <td>0</td>\n",
       "      <td>['Schweiz']</td>\n",
       "      <td>['Sorry, the page you are looking for is curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>5741</td>\n",
       "      <td>12</td>\n",
       "      <td>715</td>\n",
       "      <td>164</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title     author     date nmbr_comments       themes  \\\n",
       "count       7232       7232     7232          7232         7232   \n",
       "unique      7203         60     7211           288         4000   \n",
       "top     no_title  no_author  no_date             0  ['Schweiz']   \n",
       "freq          15       5741       12           715          164   \n",
       "\n",
       "                                                  article  \n",
       "count                                                7232  \n",
       "unique                                               7214  \n",
       "top     ['Sorry, the page you are looking for is curre...  \n",
       "freq                                                    9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"watson_schweiz.csv\",sep = \";\") \n",
    "display(data.head(5))\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first look, we see already some issues, so lets further visualise the data to see what's next. Since I'm only interested in article text and the author, I will only have a look at these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adrian Müller</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camille Kündig</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Christoph Bernet</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fabio Vonarburg</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helene Obrist</th>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline Büchi</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leo Helfenberger</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peter Blunschi</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sarah Serafini</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>William Stern</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article\n",
       "author                   \n",
       "Adrian Müller          63\n",
       "Camille Kündig        113\n",
       "Christoph Bernet      149\n",
       "Fabio Vonarburg       104\n",
       "Helene Obrist         152\n",
       "Jacqueline Büchi      155\n",
       "Leo Helfenberger       52\n",
       "Peter Blunschi        133\n",
       "Sarah Serafini         99\n",
       "William Stern          60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_reduced = data.filter(items=['author', 'article'])\n",
    "# filter no_author\n",
    "data_reduced = data_reduced[-data_reduced['author'].str.contains(\"no_author\")]\n",
    "# authors_article = data_reduced.groupby('author').count().reset_index()\n",
    "# for simplicity I will reduce the number of authors. I set a threshold of minimum 50 articles \n",
    "\n",
    "g = data_reduced.groupby('author')\n",
    "data_reduced = g.filter(lambda x: len(x) > 50).reset_index(drop = True)\n",
    "display(data_reduced.groupby('author').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already way better - Only the authors with more than 50 articles are left. The next steps contain the preparation of the text itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "for index,s in enumerate(data_reduced[\"article\"]):\n",
    "    exclude = set(string.punctuation)\n",
    "    data_reduced[\"article\"][index] = ''.join(ch for ch in s if ch not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the lemmatization on the whole dataset, I remove the Stopwords. It leaves less words to process\n",
    "\n",
    "Stopwords are usually words that do not really contain much valuable information, but frequently occur, about a text.\n",
    "\n",
    "Examples:\n",
    "- die\n",
    "- dort\n",
    "- zu\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gwehrm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dowloading the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifiy german\n",
    "from nltk.corpus import stopwords\n",
    "# and check them\n",
    "stopwords.words('german')[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,article in enumerate(data_reduced[\"article\"]):\n",
    "    data_reduced[\"article\"][index]=  tagger.tag([word for word in article.split() if word.lower() not in stopwords.words('german')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare for the lemmatization - I followed the steps according to https://github.com/WZBSocialScienceCenter/germalemma/blob/master/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dowloaded corpus \n",
    "corp = nltk.corpus.ConllCorpusReader('C:\\\\Users\\\\gwehrm\\\\Documents', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                     encoding='utf-8')\n",
    "\n",
    "tagged_sents = list(corp.tagged_sents())\n",
    "random.shuffle(tagged_sents)\n",
    "\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "# from ClassifierBasedGermanTagger\n",
    "#train the classifier ()\n",
    "tagger = cl.ClassifierBasedGermanTagger(train=train_sents)\n",
    "\n",
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "\n",
    "accuracy = tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'ClassifierBasedGermanTagger' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ab75f941e218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nltk_german_classifier_data.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'ClassifierBasedGermanTagger' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# to write the trained tagger on the disk that its not necessary to train it each time\n",
    "\n",
    "# with open('nltk_german_classifier_data.pickle', 'wb') as f:\n",
    "#     pickle.dump(tagger, f, protocol=2)\n",
    "    #to load\n",
    "with open('nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "# passing the word and the POS tag \n",
    "for index, tos in enumerate(data_reduced[\"article\"]):\n",
    "    article=[]\n",
    "    for i in tos:\n",
    "        try:\n",
    "            word, N = i\n",
    "            lemma = lemmatizer.find_lemma(word,N)\n",
    "            article.append(lemma)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    data_reduced.iloc[index,1] = article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_reduced[\"author\"]\n",
    "X = data_reduced[\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(X):\n",
    "    X[index] = ' '.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ntamara Funiciello JusoPräsidentin Zielscheibe...\n",
       "1       aktuell Kriminalstatistik zeigen deutlich besc...\n",
       "2       Tatort Mehrfamilienhaus Zürich Wipkingen Tatbe...\n",
       "3       KlimastreikBewegung feiern ÖkoParteien erster ...\n",
       "4       Parteipräsident Konrad Langhart SVP links Hans...\n",
       "                              ...                        \n",
       "1075    nerich Hess verstossen bilden keystonen schlit...\n",
       "1076    nvor ländlich Gebiet Christoph Blocher Medieni...\n",
       "1077    ndies Schild hängen Samstag Sonntag Poolbereic...\n",
       "1078    nberuflich ermorden David Bild kämpfen befinde...\n",
       "1079    nwar SwissMitarbeitern beliebt Bild KEYSTONE J...\n",
       "Name: article, Length: 1080, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                   ,test_size=0.2, random_state=1234)\n",
    "\n",
    "# defining the bag-of-words transformer on the text-processed corpus # i.e., text_process() declared in II is executed...\n",
    "bow_transformer=CountVectorizer().fit(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_train=bow_transformer.transform(X_train)#ONLY TRAINING DATA\n",
    "\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_test=bow_transformer.transform(X_test)#TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# instantiating the model with Multinomial Naive Bayes..\n",
    "model = MultinomialNB()\n",
    "# training the model...\n",
    "model = model.fit(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49537037037037035"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(text_bow_train, y_train)\n",
    "model.score(text_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   Adrian Müller       1.00      0.05      0.10        19\n",
      "  Camille Kündig       0.92      0.44      0.59        25\n",
      "Christoph Bernet       0.44      0.90      0.59        30\n",
      " Fabio Vonarburg       0.62      0.29      0.40        17\n",
      "   Helene Obrist       0.32      0.52      0.40        21\n",
      "Jacqueline Büchi       0.45      0.69      0.55        42\n",
      "Leo Helfenberger       1.00      0.10      0.18        10\n",
      "  Peter Blunschi       0.84      0.67      0.74        24\n",
      "  Sarah Serafini       0.36      0.31      0.33        16\n",
      "   William Stern       1.00      0.08      0.15        12\n",
      "\n",
      "        accuracy                           0.50       216\n",
      "       macro avg       0.70      0.41      0.40       216\n",
      "    weighted avg       0.65      0.50      0.46       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "# getting the predictions of the Validation Set...\n",
    "predictions = model.predict(text_bow_test)\n",
    "# getting the Precision, Recall, F1-Score\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 1  0  6  0  6  6  0  0  0  0]\n",
      " [ 0 11  1  1  7  4  0  0  1  0]\n",
      " [ 0  0 27  0  1  2  0  0  0  0]\n",
      " [ 0  0  4  5  5  2  0  0  1  0]\n",
      " [ 0  1  4  1 11  4  0  0  0  0]\n",
      " [ 0  0  8  0  1 29  0  3  1  0]\n",
      " [ 0  0  0  0  2  6  1  0  1  0]\n",
      " [ 0  0  4  0  0  2  0 16  2  0]\n",
      " [ 0  0  5  0  1  5  0  0  5  0]\n",
      " [ 0  0  3  1  0  4  0  0  3  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn! You can identify journalists based on their articles. Some better than others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
